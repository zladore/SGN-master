#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import torch.optim as optim
import os
import json
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import math
import datetime
import shutil

from loader.data_loader import ParticleDataLoader
from loader.ParticleDataset import ParticleDataset
from models.model_builder import build_model
from training import train_epoch
from torch.optim.lr_scheduler import LambdaLR

# ===============================================================
# ğŸ”¹ é¢„æµ‹å¯è§†åŒ–ï¼ˆåå½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰
# ===============================================================
@torch.no_grad()
def plot_predictions(model, data_loader, device, epoch, label_mean, label_std, save_dir="results"):
    model.eval()

    for batch in data_loader:
        images = batch["image"].to(device)
        labels = batch["label"].to(device)

        outputs = model(images)
        preds = outputs.detach().cpu().numpy()
        trues = labels.detach().cpu().numpy()
        break

    # åå½’ä¸€åŒ–
    preds_denorm = preds * label_std + label_mean
    trues_denorm = trues * label_std + label_mean

    preds_flat = preds_denorm.flatten()
    trues_flat = trues_denorm.flatten()

    mse = np.mean((preds_flat - trues_flat) ** 2)
    mae = np.mean(np.abs(preds_flat - trues_flat))

    plt.figure(figsize=(10, 5))
    plt.plot(preds_flat, label="Prediction")
    plt.plot(trues_flat, label="Ground Truth")
    plt.title(f"Epoch {epoch} | MSE={mse:.4f} | MAE={mae:.4f}")
    plt.legend()
    plt.tight_layout()

    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, f"pred_vs_truth_epoch_{epoch}.png")
    plt.savefig(save_path)
    plt.close()

    print(f"âœ… å·²ä¿å­˜é¢„æµ‹å¯¹æ¯”å›¾: {save_path}")


# ===============================================================
# ğŸ”¹ Cosine Warmup è°ƒåº¦å™¨
# ===============================================================
def build_scheduler(optimizer, config):
    sched_cfg = config.get("scheduler", {})
    name = sched_cfg.get("name", None)

    if name == "CosineAnnealingWarmup":
        warmup_epochs = sched_cfg.get("warmup_epochs", 10)
        max_epochs = sched_cfg.get("max_epochs", 200)
        min_lr = sched_cfg.get("min_lr", 1e-6)
        base_lr = optimizer.param_groups[0]["lr"]

        def lr_lambda(epoch):
            if epoch < warmup_epochs:
                return epoch / warmup_epochs
            else:
                cosine_decay = 0.5 * (1 + math.cos(
                    math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)
                ))
                return cosine_decay * (1 - min_lr / base_lr) + (min_lr / base_lr)

        print(f"âœ… ä½¿ç”¨ CosineAnnealingWarmup è°ƒåº¦å™¨")
        return LambdaLR(optimizer, lr_lambda)

    print("âš ï¸ æœªå®šä¹‰è°ƒåº¦å™¨ï¼Œå°†ä½¿ç”¨é»˜è®¤å­¦ä¹ ç‡")
    return None


# ===============================================================
# ğŸ”¹ ä¸»ç¨‹åº
# ===============================================================
def main():
    # ========= 1. åŠ è½½é…ç½® =========
    config_path = "data/particle_config/particle_config.json"
    with open(config_path, 'r') as f:
        config = json.load(f)

    # ========= 2. è®¾å¤‡ =========
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    # ========= 3. DataLoaderï¼ˆè‡ªåŠ¨è®¡ç®—å½’ä¸€åŒ–ï¼‰=========
    data_module = ParticleDataLoader(config)
    train_loader, _, _ = data_module.get_loaders()
    train_dataset, _, _ = data_module.get_datasets()

    # ========= 4. æœ€æ–°å½’ä¸€åŒ–å‚æ•° =========
    # ç›´æ¥ä»è®­ç»ƒé›†åŠ¨æ€è·å–å½’ä¸€åŒ–å‚æ•°ã€‚
    # norm_params = train_dataset.get_normalization_params()
    # label_mean = norm_params["label_mean"]
    # label_std = norm_params["label_std"]
    # ä» æ–‡ä»¶åŠ è½½å½’ä¸€åŒ–å‚æ•°
    norm_path = "data/norm_params/normalization_params.json"
    norm_params = ParticleDataset.load_normalization_params(norm_path)
    label_mean = np.array(norm_params["label_mean"])
    label_std = np.array(norm_params["label_std"])

    # ========= 5. æ„å»ºæ¨¡å‹ =========
    model = build_model(config.get("model", {})).to(device)
    print("âœ… æ¨¡å‹å·²æ„å»ºå®Œæˆ")

    # ========= 6. æŸå¤±ä¸ä¼˜åŒ–å™¨ =========
    criterion = torch.nn.SmoothL1Loss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=config.get("optimizer", {}).get("lr", 1e-4),
        weight_decay=1e-5
    )

    scheduler = build_scheduler(optimizer, config)

    # ========= 7. è¾“å‡ºç›®å½• =========
    timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    exp_dir = f"results/exp_{timestamp}"
    os.makedirs(exp_dir, exist_ok=True)
    os.makedirs("checkpoints", exist_ok=True)

    shutil.copy(config_path, os.path.join(exp_dir, "config_used.json"))
    with open(os.path.join(exp_dir, "model_summary.txt"), "w") as f:
        f.write(str(model))

        # ========= 8. è®­ç»ƒå¾ªç¯ =========
    num_epochs = config.get("training", {}).get("n_epochs", 50)

    # âœ… è®­ç»ƒæ—¥å¿—
    history = {"epoch": [], "train_loss": [], "mse_real": [], "mae_real": [], "lr": []}

    # âœ…ã€ä½ è¦æ±‚çš„ï¼šæœ€ä¼˜æ¨¡å‹ä¿å­˜æœºåˆ¶ã€‘
    BEST_METRIC = "loss"  # å¯æ”¹ä¸º "loss" / "mae" / "mse"
    best_value = float("inf")
    best_ckpt_path = "checkpoints/best_model.pth"
    print(f"â­ ä½¿ç”¨æœ€ä¼˜æŒ‡æ ‡ä¿å­˜ç­–ç•¥ï¼š{BEST_METRIC}")

    for epoch in range(1, num_epochs + 1):
        print(f"\n========== Epoch {epoch}/{num_epochs} ==========")

        # è®­ç»ƒ
        train_loss, (mse_real, mae_real) = train_epoch(
            epoch, train_loader, model, criterion, optimizer, device
        )

        if scheduler is not None:
            scheduler.step()

        lr = optimizer.param_groups[0]["lr"]

        print(f"ğŸ“‰ å½“å‰å­¦ä¹ ç‡: {lr:.8f}")
        print(f"ğŸ“Š Loss={train_loss:.6f},  MSE={mse_real:.6f},  MAE={mae_real:.6f}")

        # è®°å½•æ—¥å¿—
        history["epoch"].append(epoch)
        history["train_loss"].append(train_loss)
        history["mse_real"].append(mse_real)
        history["mae_real"].append(mae_real)
        history["lr"].append(lr)

        # ========= âœ…æœ€ä¼˜æ¨¡å‹åˆ¤å®šé€»è¾‘ =========
        if BEST_METRIC == "loss":
            current = train_loss
        elif BEST_METRIC == "mae":
            current = mae_real
        elif BEST_METRIC == "mse":
            current = mse_real
        else:
            raise ValueError("BEST_METRIC å¿…é¡»æ˜¯ loss / mae / mse")

        # âœ… å½“å‰æŒ‡æ ‡æ›´ä¼˜ â†’ ä¿å­˜
        if current < best_value:
            print(f"ğŸ’¾ æœ€ä¼˜æ¨¡å‹æ›´æ–° {best_value:.6f} â†’ {current:.6f} (Epoch {epoch})")
            best_value = current
            best_ckpt_path = f"checkpoints/best_model_epoch_{epoch}.pth"
            torch.save(model.state_dict(), best_ckpt_path)

        # ========= å¯è§†åŒ–ï¼ˆæ¯10è½®ï¼‰ =========
        if epoch % 10 == 0 or epoch == num_epochs:
            plot_predictions(model, train_loader, device, epoch, label_mean, label_std, exp_dir)

    print(f"\nâœ… è®­ç»ƒç»“æŸï¼Œæœ€ä½³ {BEST_METRIC} = {best_value:.6f}")
    print(f"âœ… æœ€ä¼˜æ¨¡å‹ä¿å­˜åœ¨ï¼š{best_ckpt_path}")

    # ========= 9. ä¿å­˜æ—¥å¿— & æ›²çº¿ =========
    df = pd.DataFrame(history)
    df.to_csv(os.path.join(exp_dir, "training_log.csv"), index=False)

    plt.figure(figsize=(8, 6))
    plt.plot(df["epoch"], df["train_loss"], marker='o')
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training Loss Curve")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(exp_dir, "train_loss_curve.png"))
    plt.close()

    plt.figure(figsize=(8, 5))
    plt.plot(df["epoch"], df["lr"], color='purple')
    plt.xlabel("Epoch")
    plt.ylabel("Learning Rate")
    plt.title("Learning Rate Schedule")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(exp_dir, "lr_curve.png"))
    plt.close()

    print(f"ğŸ“Š æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {exp_dir}")


if __name__ == "__main__":
    main()
