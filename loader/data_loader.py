from __future__ import print_function, division

import json
import os
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision import transforms

from loader.ParticleDataset import ParticleDataset

class ParticleDataLoader:
    """ç²’å­æ•°æ®åŠ è½½å™¨ - ç®¡ç†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨"""

    def __init__(self, config):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰é…ç½®å‚æ•°
        """
        self.config = config
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None
        self.train_loader = None
        self.val_loader = None
        self.test_loader = None
        self.input_dir = config['dataset']['input_dir']
        self.output_dir = config['dataset']['output_dir']

        # æ„å»ºæ•°æ®åŠ è½½å™¨
        self.build()

    def _resolve_file_paths(self, file_list):
        """å°†ç›¸å¯¹è·¯å¾„è§£æä¸ºç»å¯¹è·¯å¾„"""
        resolved_list = []
        for file_info in file_list:
            # å¤„ç†è¾“å…¥æ–‡ä»¶è·¯å¾„
            if not os.path.isabs(file_info["image"]):
                image_path = os.path.join(self.input_dir, os.path.basename(file_info["image"]))
            else:
                image_path = file_info["image"]

            # å¤„ç†è¾“å‡ºæ–‡ä»¶è·¯å¾„
            if not os.path.isabs(file_info["label"]):
                label_path = os.path.join(self.output_dir, os.path.basename(file_info["label"]))
            else:
                label_path = file_info["label"]

            resolved_list.append({
                "image": image_path,
                "label": label_path
            })
        return resolved_list

    def build(self):
        """æ„å»ºæ•°æ®åŠ è½½å™¨"""
        print("Building data loaders...")

        # --- è§£ææ–‡ä»¶è·¯å¾„ ---
        train_files = self._resolve_file_paths(self.config.get("training_filenames", []))
        val_files = self._resolve_file_paths(self.config.get("validation_filenames", []))
        test_files = self._resolve_file_paths(self.config.get("test_filenames", []))

        if not train_files:
            raise ValueError("âŒ è®­ç»ƒé›†æ–‡ä»¶åˆ—è¡¨ä¸ºç©ºï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")

        print(f"æ‰¾åˆ° {len(train_files)} ä¸ªè®­ç»ƒæ ·æœ¬, {len(test_files)} ä¸ªæµ‹è¯•æ ·æœ¬")

        # --- æ•°æ®åŠ è½½å™¨å‚æ•° ---
        dataset_config = self.config.get("dataset", {})
        data_loader_config = dataset_config.get("data_loader", {})
        batch_size = data_loader_config.get("batch_size", 4)
        shuffle = data_loader_config.get("shuffle", True)
        num_workers = data_loader_config.get("num_workers", 0)

        training_config = self.config.get("training", {})
        training_batch_size = training_config.get("batch_size", batch_size)
        val_batch_size = training_config.get("validation_batch_size", batch_size)

        # --- æ„å»ºè®­ç»ƒé›† ---
        self.train_dataset = ParticleDataset(
            filenames=train_files,
            normalize_input=True,
            normalize_label=True,
        )

        # è·å–è®­ç»ƒé›†çš„ input å’Œ labelå½’ä¸€åŒ–å‚æ•°
        norm_params = self.train_dataset.get_normalization_params()

        # --- éªŒè¯é›† (å…±äº«è®­ç»ƒé›†çš„ input å’Œ label å½’ä¸€åŒ–å‚æ•°) ---
        self.val_dataset = ParticleDataset(
            filenames=val_files,
            normalize_input=True,
            normalize_label=True,
            **norm_params
        )

        # --- æµ‹è¯•é›† (å…±äº«è®­ç»ƒé›†çš„ input å’Œ label å½’ä¸€åŒ–å‚æ•°) ---
        self.test_dataset = ParticleDataset(
            filenames=test_files,
            normalize_input=True,
            normalize_label=True,
            **norm_params
        )

        # --- DataLoader ---
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=training_batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=self.collate_fn
        )

        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=val_batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=self.collate_fn
        )

        self.test_loader = DataLoader(
            self.test_dataset,
            batch_size=val_batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=self.collate_fn
        )

        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(self.train_dataset)} ä¸ªæ ·æœ¬, æ‰¹æ¬¡å¤§å°: {training_batch_size}")
        print(f"  éªŒè¯é›†: {len(self.val_dataset)} ä¸ªæ ·æœ¬, æ‰¹æ¬¡å¤§å°: {val_batch_size}")
        print(f"  æµ‹è¯•é›†: {len(self.test_dataset)} ä¸ªæ ·æœ¬, æ‰¹æ¬¡å¤§å°: {val_batch_size}")

        # --- ä¿å­˜ input çš„å½’ä¸€åŒ–å‚æ•° ---
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼šä»å½“å‰æ–‡ä»¶ä½ç½®åˆ° data/norm_params
        norm_params_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'norm_params')
        os.makedirs(norm_params_dir, exist_ok=True)

        # ä¿å­˜åˆ° data/norm_params/ ç›®å½•ä¸‹
        norm_params_path = os.path.join(norm_params_dir, 'normalization_params.json')
        self.train_dataset.save_normalization_params(norm_params_path)

    def collate_fn(self, batch):
        """è‡ªå®šä¹‰æ‰¹æ¬¡å¤„ç†å‡½æ•°"""
        valid_batch = [item for item in batch if not torch.isnan(item["image"]).any()]

        if len(valid_batch) == 0:
            # å¦‚æœæ•´ä¸ªæ‰¹æ¬¡éƒ½æœ‰é”™è¯¯ï¼Œè¿”å›ç©ºæ‰¹æ¬¡
            return {
                "image": torch.empty(0, 4, 3, 250, 30),
                "label": torch.empty(0, 250),
                "filename": []
            }

        # æå–æœ‰æ•ˆæ•°æ®
        images = torch.stack([item["image"] for item in valid_batch])
        labels = torch.stack([item["label"] for item in valid_batch])
        filenames = [item["filename"] for item in valid_batch]

        return {
            "image": images,
            "label": labels,
            "filename": filenames
        }

    def get_loaders(self):
        """è·å–æ‰€æœ‰æ•°æ®åŠ è½½å™¨"""
        return self.train_loader, self.val_loader, self.test_loader

    def get_datasets(self):
        """è·å–æ‰€æœ‰æ•°æ®é›†"""
        return self.train_dataset, self.val_dataset, self.test_dataset


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        config = json.load(f)
    return config


if __name__ == "__main__":
    # ===============================
    # 1ï¸âƒ£ åŠ è½½é…ç½®æ–‡ä»¶
    # ===============================
    config_path = "/home/hqu/PycharmProjects/SGN-master/data/particle_config/particle_config.json"
    config = load_config(config_path)

    # ===============================
    # 2ï¸âƒ£ æ„å»º DataLoader
    # ===============================
    loader = ParticleDataLoader(config)
    train_loader, val_loader, test_loader = loader.get_loaders()
    train_dataset, _, _ = loader.get_datasets()

    # ===============================
    # 3ï¸âƒ£ æ‰“å°å½’ä¸€åŒ–å‚æ•°
    # ===============================
    norm_params = train_dataset.get_normalization_params()
    print("\nğŸ“Š ==== å½’ä¸€åŒ–å‚æ•°æ£€æŸ¥ ====")
    print("Input mean:", np.round(norm_params["input_mean"], 5))
    print("Input std :", np.round(norm_params["input_std"], 5))
    print("Label mean shape:", norm_params["label_mean"].shape)
    print("Label std  shape:", norm_params["label_std"].shape)

    # ===============================
    # 4ï¸âƒ£ æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    # ===============================
    for batch_idx, batch in enumerate(train_loader):
        print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_idx}:")
        print(f"  è¾“å…¥å½¢çŠ¶: {batch['image'].shape}")   # (batch_size, 4, 3, 250, 30)
        print(f"  æ ‡ç­¾å½¢çŠ¶: {batch['label'].shape}")   # (batch_size, 250)
        print(f"  æ–‡ä»¶åæ ·ä¾‹: {batch['filename'][:2]}")

        # --- æ£€æŸ¥å½’ä¸€åŒ–æ•ˆæœ ---
        inputs = batch["image"].numpy()
        labels = batch["label"].numpy()

        print(f"  ğŸ” è¾“å…¥å‡å€¼(åº”â‰ˆ0): {inputs.mean():.4f}")
        print(f"  ğŸ” è¾“å…¥æ ‡å‡†å·®(åº”â‰ˆ1): {inputs.std():.4f}")
        print(f"  ğŸ” æ ‡ç­¾å‡å€¼(åº”â‰ˆ0): {labels.mean():.4f}")
        print(f"  ğŸ” æ ‡ç­¾æ ‡å‡†å·®(åº”â‰ˆ1): {labels.std():.4f}")

        # --- æ£€æŸ¥åå½’ä¸€åŒ–æ•ˆæœ ---
        denorm_labels = train_dataset.denormalize_label(labels)

        print(f"  ğŸ”„ åå½’ä¸€åŒ–åæ ‡ç­¾å‡å€¼: {denorm_labels.mean():.4f}")
        print(f"  ğŸ”„ åå½’ä¸€åŒ–åæ ‡ç­¾æ ‡å‡†å·®: {denorm_labels.std():.4f}")

        # --- æ”¹è¿›éªŒè¯é€»è¾‘ ---
        global_mean = norm_params["label_mean"].mean()
        mean_diff = abs(denorm_labels.mean() - global_mean)
        print(f"  ğŸ“ å‡å€¼å·®è·: {mean_diff:.2f}")

        if mean_diff < 0.5 * norm_params["label_std"].mean():
            print("âœ… æ ‡ç­¾åå½’ä¸€åŒ–åˆ†å¸ƒåˆç†")
        else:
            print("âš ï¸ å½“å‰æ‰¹æ¬¡åˆ†å¸ƒåç¦»æ•´ä½“ï¼ˆä½†ä¸ä¸€å®šæ˜¯é”™è¯¯ï¼‰")

        break  # åªå–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡

